{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning for Stock Market Predictions in 25 Lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial is meant as a walkthrough for you to learn a bit about how we can apply some great libraries that are freely available to do some basic work in predicting the movements of stock prices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over the course of this tutorial, we'll be trying to predict whether in the\n",
    "next time step, a certain product will go up or down. To keep things simple,\n",
    "we'll use the S&P 500 product, SPX. For features, we'll stick with the \n",
    "Open, High, Low, and Close prices, and volume."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use an awesome library called XGBoost (XGB) to do most of the heavy \n",
    "lifting ML stuff, as well as a few other libraries to do some data processing.\n",
    "XGBoost is an awesome library that's used a lot in competitions for ML and \n",
    "data mining online, and for good reason. It has awesome performance, is\n",
    "lightning fast, and is somewhat easier to work with an understand, relative\n",
    "to other machine learning models like Neural Network. It uses decision trees\n",
    "and a process called boosting, that iteratively improves the performance of \n",
    "a decision tree based model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to note that this tutorial is not something you should deploy\n",
    "to start trading your hard earned cash, and the performance is pretty poor.\n",
    "Each product trades differently, and requires different considerations. It is\n",
    "far more important to consider what features to use, and how to use them, than\n",
    "it is worrying about the model itself. Also, as with any model, it is almost\n",
    "impossible to gain 100% accuracy, especially in trading, where strategies \n",
    "always adapt. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll Need These Libraries\n",
    "\n",
    "    Python 3.5\n",
    "    pandas 0.22.0\n",
    "    NumPy 1.14.2\n",
    "    Scikit-Learn 0.19.1\n",
    "    XGBoost 0.71"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries we need\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "import sklearn.preprocessing as sk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first step is to bring in our data, and convert it to a format that we can\n",
    "interpret and manipulate within Python, specifically with the Pandas library. \n",
    "We have a CSV of OHLC prices and Volume that I've pulled from online. As long \n",
    "as you can download daily OHLC and Volume going back a few years, you should\n",
    "have some basic data to start training your model. Here we're using OHLC and \n",
    "Volume as our features, just it's really important to choose the best available\n",
    "features to you if you ever want to improve your model's performance. You can\n",
    "also apply modifications to your data if you want to further improve your\n",
    "models accuracy as well. We'll engineer a very basic feature here, which is\n",
    "the return of close prices from day to day. This model will perform poorly as\n",
    "we are using raw values of OHLC and Volume (which tend to increase overtime),\n",
    "and as such, we won't make meaningful relations from the data - but this is\n",
    "where you have to consider, what features make sense. As long as you can\n",
    "understand the model, and make great features, you can do well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data is pulled from a file \n",
    "data = pd.read_csv(r\"SPX.csv\", index_col=0)\n",
    "data[\"Close Return\"] = data[\"Close\"].pct_change()\n",
    "# Convert the index to date time format, so that it is easier to work with\n",
    "data.index = pd.to_datetime(data.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning and Labelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our data, the next step is to work on some really basic\n",
    "cleaning and labelling. Here, we'll just drop any rows that have a missing, or\n",
    "NaN value, and keep only those that have all values available. It's important\n",
    "to note that you can have missing values in a boosted tree model when using \n",
    "XGBoost, but it's poor practise as we cannot use this same data if we want to\n",
    "use something like a neural network. \n",
    "We're also going to assign a label. A label is just something that is the \n",
    "'correct value,' that we're trying to predict. Here, we know that if the next\n",
    "week's price is higher, we'll want to buy this week. If it's lower, we should\n",
    "sell. Labels can get a lot more complicated than this, but we're just going to\n",
    "do this simply. We'll also save our raw data to a CSV in case we ever want to \n",
    "use it for another model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop any NaN rows\n",
    "data= data.dropna(how=\"any\")\n",
    "#Label assigned, 0 for sell, and 1 for buy\n",
    "data[\"Label\"] = np.where(data[\"Close\"] > (data[\"Close\"].shift(-1)), 0, 1)\n",
    "# Data saved to a CSV in our directory\n",
    "pd.DataFrame.to_csv(data, \"SPX_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Train, Test and Normalizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A machine learning model needs a lot of data to effectively train and make\n",
    "meaningful predictions, and some data to test the results of training on. When\n",
    "we pass in our data, we split it into a train set and a test set. The train \n",
    "will be used to train and improve the model through each boosting iteration,\n",
    "and the test set will be used later to evaluate. \n",
    "Although you don't need to normalize, we're going to do it here as it is an\n",
    "important process for almost all other ML applications. Normalizing simply\n",
    "takes your data and scales it to a normal range. Usually, this range is\n",
    "between 0 and 1, however, it can also be from -1 to 1. We set our normalizing\n",
    "parameters based on the Std dev and mean of our training set, so we may get\n",
    "values above 1 in our test set. There are numerous reasons for this, which I\n",
    "won't get into. We can use a library called Scikit-learn for this, which makes\n",
    "things really easy. Here, we assign our maximum value 1, and minimum 0. \n",
    "Lastly, we'll want to create an X and y component of both train and test. In \n",
    "both, the X component is the data our model can see (OHLC and Volume), and the\n",
    "y label is the data our model is supposed to try and predict. We'll split the\n",
    "data by dropping the label for X, and only including the label for y. We also\n",
    "want to convert this into a DMatrix format, a specific data structure that XGB\n",
    "can read quickly. This step is exclusive to XGB, and not required in other ML\n",
    "approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 7)) while a minimum of 1 is required by MinMaxScaler.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-2128333fecdc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Initializing a normalizer from Scikit-Learn, called sk here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMinMaxScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m# Normalizing the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0;31m# Reset internal state before fitting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         X = check_array(X, copy=self.copy, warn_on_dtype=True,\n\u001b[0;32m--> 334\u001b[0;31m                         estimator=self, dtype=FLOAT_DTYPES)\n\u001b[0m\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \u001b[0mdata_min\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    460\u001b[0m                              \u001b[0;34m\" minimum of %d is required%s.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m                              % (n_samples, shape_repr, ensure_min_samples,\n\u001b[0;32m--> 462\u001b[0;31m                                 context))\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_features\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 7)) while a minimum of 1 is required by MinMaxScaler."
     ]
    }
   ],
   "source": [
    "# Defining train and test\n",
    "train_data = data['1992-01-01':'2004-12-31'].copy()\n",
    "test_data = data['2005-01-01':'2006-01-01'].copy()\n",
    "# Initializing a normalizer from Scikit-Learn, called sk here\n",
    "scaler = sk.MinMaxScaler()\n",
    "scaler.fit(train_data)\n",
    "# Normalizing the data\n",
    "train_data.loc[:, train_data.columns] = scaler.transform(train_data)\n",
    "test_data.loc[:,test_data.columns] = scaler.transform(test_data)\n",
    "# Train and Test Data and Label Assignment\n",
    "X_train = train_data.drop(\"Label\",1)\n",
    "y_train = train_data[\"Label\"]\n",
    "X_test = test_data.drop(\"Label\",1)\n",
    "y_test = test_data[\"Label\"]\n",
    "# Split into DMatrix, format of XGBoost\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've processed some of our data, we can finally start with the ML,\n",
    "what we've been waiting for!\n",
    "We're using the full Python version of XGB, but you can also use the \n",
    "Scikit-Learn version if you're more comfortable with that. \n",
    "First, we'll define some basic parameters. Parameters are just some basic rules\n",
    "and instructions for our model to follow while it's fitting the model, all to\n",
    "make sure that it trains how we want it to. We're specifying that the model\n",
    "shouldn't go deeper than 5 layers (it can go less than this), that that it \n",
    "should use a learning rate of 0.1 to conduct gradient descent. It's also\n",
    "going to be using binary logistic classification for it's objective. \n",
    "Next, we specify the number of rounds. The more rounds, the more overfit the\n",
    "model, but too few means not enough learning was done. Each round is an\n",
    "iterative improvement of a previous tree model, and helps our model learn. We\n",
    "will use 100 rounds here. It's like epochs in a neural network!\n",
    "Then, the big show! We'll train our model by passing in our parameters, our\n",
    "train data, and for a specified number of rounds. This will take a few seconds\n",
    "to a few minutes to process depending on how much data you've pulled. This \n",
    "trained model can then be used to predict output labels based on our inputs in\n",
    "the dtrain matrix. \n",
    "After we've done our predictions, we'll make an array with them. The numbers \n",
    "won't come in as clean 0 and 1 values as the tree cannot perfectly fit them. It\n",
    "will ouput the number close to what it expects to be the real value, so we\n",
    "can round anything above 0.5 to 1, and anything below to 0, as our buy and sell\n",
    "signals respectively. \n",
    "After we've done this, we can plug it into an accuracy measure which will\n",
    "benchmark how accurate we were in our predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dtrain' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-3458c0669cad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0miters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Create and train the model given our parameters, training set, and rounds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m# Create predictions with out test set from the model we created\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dtrain' is not defined"
     ]
    }
   ],
   "source": [
    "# Parameters for this model. Maximum of 5 layers, learning rate of 0.1, and \n",
    "# binary logistic classification which is used for two class predictions\n",
    "param = {'max_depth':5, 'eta':0.1, 'objective':'binary:logistic'}\n",
    "# Go through 100 rounds of boosting\n",
    "iters = 100\n",
    "# Create and train the model given our parameters, training set, and rounds\n",
    "model = xgb.train(param, dtrain, iters)\n",
    "# Create predictions with out test set from the model we created\n",
    "preds = model.predict(dtest)\n",
    "# Round the numbers off to 0 and 1 for buy and sell\n",
    "best_preds = np.asarray([np.round(line) for line in preds])\n",
    "# Test the accuracy of our model against the true values\n",
    "accuracytest = accuracy = (accuracy_score(y_test, best_preds))\n",
    "# Print the accuracy\n",
    "print(\"Accuracy on test set was {0}%. The tree went through {1} iterations\" \n",
    "      .format(accuracytest*100, iters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output: Accuracy on test set was 49.2%. The tree went through 100 iterations\n",
    "\n",
    "Done! In 25 lines of code and about 175 lines of comments/imports. \n",
    "Congratulations! You've succesfully trained and tested a stock prediction \n",
    "algorithm using XGBoost. Keep in mind, this is just one way of tackling the \n",
    "problem of stock market predictions, and there are countless ways to approach\n",
    "it. This tutorial shouldn't be used to make investment decisions, but can\n",
    "be a great resource to start exploring more complex and accurate models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
